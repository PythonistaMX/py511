{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![img/pythonista.png](img/pythonista.png)](https://www.pythonista.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a *Dataframes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Intro a Dataframes\").getOrCreate()\n",
    "ct = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *dataframes* son un concepto compartido entre plataformas como *R*, *Pandas* y *Scala*. Son estructuras tabulares en las que todos los datos de una columna comparten el mismo tipado. Cada columna tiene un título y cada renglón tiene un índice. A la descripción de los tipos de datos de cada columna de un *dataframe* se le conoce como esquema (*schema*).\n",
    "\n",
    "*PySPark* tiene la capacidad de poder manejar *dataframes* tanto de *Pandas* como de *Spark* e incluso cuenta con una [*API*](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) que optimiza la interacción entre ambos tipos de *dataframes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los *dataframes* de *Spark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *dataframe* de *Spark* es un objeto instanciado de la clase [```pyspark.sql.DataFrame```](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de *dataframes*.\n",
    "\n",
    "La función [```spark.createDataFrame()```](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html) permite crear *dataframes* a partir de objetos que se ingresan como argumentos.\n",
    "\n",
    "\n",
    "```pyspark\n",
    "df = spark.createDataFrame(data=<obj>, <títulos columnas>, schema=<esquema>)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ```<obj>``` es un objeto que represente una estructura tabular el cual puede ser:\n",
    "    * Una colección de *Python*.\n",
    "    * Un *dataframe* de *Pandas*.\n",
    "    * Un *RDD* de *Spark*.\n",
    "* ```<titulos columnas>``` es una lista de cadenas de caracteres que corresponden al título de cada columna.\n",
    "* ```<esquema>``` es una estructura de tipos de datos de *Spark* que describe el tipado de cada columna.\n",
    "    \n",
    "Cabe hacer notar que los *dataframes* de *Spark* se construyen de forma perezosa, por lo que aún cuando sean definidos, estos no serán creados hasta que sean requeridos.\n",
    "    \n",
    "Por convención se utiliza el nombre ```df``` para designar un dataframe genérico.\n",
    "\n",
    "https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los objetos ```Row``` y ```Column```.\n",
    "\n",
    "Los *dataframes* de *Spark* están compuestos por 2 tipos de objetos.\n",
    "\n",
    "* Los objetos de tipo ```pyspark.sql.Row``` que representan a cada renglón del *dataframe*.\n",
    "* Los objetos de tipo ```pyspark.sql.Column``` que representan a cada columna del *dataframe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferencias entre *RDD* y *Dataframes*.\n",
    "\n",
    "Los *RDD* de *Apache Spark* son colecciones de datos que pueden ser crudos (incluyendo binarios), semi-estructurados (*JSON*, *XML*, *YAML*) o estructurados. Al final de cuentas, los *RDD* funcionan de forma similar a un *data lake*, mientras que los *dataframes* siempre tiene una estructura de tabla como el caso de los sistemas de *Data Warehouse*.\n",
    "\n",
    "Los *RDD* son creados desde el contexto de *Spark*, mientras que los dataframes son creados usando la *API* de *SQL*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método ```df.show()```.\n",
    "\n",
    "El método ```df.show()``` muestra los primeros ```n``` números de un *dataframe* de *Spark*.\n",
    "\n",
    "```\n",
    "df.show(<n>)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ```<n>``` es el número de renglones desplegados. El valor por defecto es ```20```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas crearán un *dataframe* de *Spark* a partir de un *dataframe* de *Pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se creará el *dataframe* de *Pandas* llamado ```pandas_df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.DataFrame({'Dirección':('Sur', 'Norte', 'Sur', 'Este'),\n",
    "              'Rumbo':('Este', 'Noroeste', 'Norte', 'Norte'),\n",
    "             'Pasajeros':(12, 24, 32, 5),\n",
    "             'Documentado':(True, None, True, False) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dirección</th>\n",
       "      <th>Rumbo</th>\n",
       "      <th>Pasajeros</th>\n",
       "      <th>Documentado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sur</td>\n",
       "      <td>Este</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Norte</td>\n",
       "      <td>Noroeste</td>\n",
       "      <td>24</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sur</td>\n",
       "      <td>Norte</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Este</td>\n",
       "      <td>Norte</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dirección     Rumbo  Pasajeros Documentado\n",
       "0       Sur      Este         12        True\n",
       "1     Norte  Noroeste         24        None\n",
       "2       Sur     Norte         32        True\n",
       "3      Este     Norte          5       False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dirección      object\n",
       "Rumbo          object\n",
       "Pasajeros       int64\n",
       "Documentado    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se creará el *dataframe* de *Spark* a partir de ```pandas_df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-----------+\n",
      "|Dirección|   Rumbo|Pasajeros|Documentado|\n",
      "+---------+--------+---------+-----------+\n",
      "|      Sur|    Este|       12|       true|\n",
      "|    Norte|Noroeste|       24|       null|\n",
      "|      Sur|   Norte|       32|       true|\n",
      "|     Este|   Norte|        5|      false|\n",
      "+---------+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-----------+\n",
      "|Dirección|   Rumbo|Pasajeros|Documentado|\n",
      "+---------+--------+---------+-----------+\n",
      "|      Sur|    Este|       12|       true|\n",
      "|    Norte|Noroeste|       24|       null|\n",
      "+---------+--------+---------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Dirección', StringType(), True), StructField('Rumbo', StringType(), True), StructField('Pasajeros', LongType(), True), StructField('Documentado', BooleanType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas creará un *dataframe* de *Spark* a partir dde un *RDD* que contiene una colección de objetos ```Row```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *RDD* de *Spark* que contiene una sucesión de objetos tipo ```Row```.\n",
    "    * El objeto ```rdd``` tiene una estructura que puede ser convertida en una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = ct.parallelize((Row('Sur', 'Este', '12', True),\n",
    "                     Row('Norte', 'Noroeste', '24', None),\n",
    "                     Row('Sur', 'Norte', '32', True),\n",
    "                     Row('Este', 'Norte', '5', False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *RDD* de *Spark* que contiene una sucesión de objetos tipo ```Row```.\n",
    "    * El objeto ```rdd_2``` NO tiene una estructura que puede ser convertida en una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2 = ct.parallelize((Row('Sur', 'Este', '12', True),\n",
    "                     1, 2, 3, 4,\n",
    "                     Row('Sur', 'Norte', '32', True),\n",
    "                     Row('Este', 'Norte', '5', False),\n",
    "                      'uno', 'dos', b'121312412413523513462156725624567'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *dataframe* a partir del *RDD* y además se ingresará como argumento el nombre de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, \n",
    "                ['Dirección', 'Rumbo', 'Pasajeros', 'Documentado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-----------+\n",
      "|Dirección|   Rumbo|Pasajeros|Documentado|\n",
      "+---------+--------+---------+-----------+\n",
      "|      Sur|    Este|       12|       true|\n",
      "|    Norte|Noroeste|       24|       null|\n",
      "|      Sur|   Norte|       32|       true|\n",
      "|     Este|   Norte|        5|      false|\n",
      "+---------+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda intentará crear un *dataframe* a partir del *RDD* ```rdd_2``` y además se ingresará como argumento el nombre de cada columna. Debido a que ```rdd_2```no contiene una estrcuctura que puede ser convertida a tabla, se genreará una excepción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd_2, \n",
    "                ['Dirección', 'Rumbo', 'Pasajeros', 'Documentado'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *dataframe* al que se le asignarán los nombres de las columnas automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+-----+\n",
      "|   _1|      _2| _3|   _4|\n",
      "+-----+--------+---+-----+\n",
      "|  Sur|    Este| 12| true|\n",
      "|Norte|Noroeste| 24| null|\n",
      "|  Sur|   Norte| 32| true|\n",
      "| Este|   Norte|  5|false|\n",
      "+-----+--------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.createDataFrame(rdd)\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de una columna de un *dataframe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible acceder a una columna de un *dataframe* usando su nombre o usando su número de índice consecutivo iniciando desde ```0```. \n",
    "\n",
    "```\n",
    "df.<Nombre Columna>\n",
    "```\n",
    "\n",
    "```\n",
    "df[<n>]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Rumbo'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Rumbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Rumbo'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El atributo ```df.schema```.\n",
    "\n",
    "El atributo ```df.schema``` contiene la estructura del *schema* del *dataframe* como una instancia de la clase ```pyspark.sql.types.StructType``` que contiene una colección de instancias de tipo ```pyspark.sql.types.StructField```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Dirección', StringType(), True), StructField('Rumbo', StringType(), True), StructField('Pasajeros', StringType(), True), StructField('Documentado', BooleanType(), True)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método ```df.printSchema()```.\n",
    "\n",
    "El método ```df.printSchema()``` despliega una cadena de caractéres describiendo el *schema* del *dataframe*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dirección: string (nullable = true)\n",
      " |-- Rumbo: string (nullable = true)\n",
      " |-- Pasajeros: string (nullable = true)\n",
      " |-- Documentado: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipado de *Spark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Spark* cuenta con distintos tipos de datos que extienden a los tipos nativos de *Scala*, *Python* y *R*. El módulo ```pyspark.sql.types``` contiene a todas las clases correspondientes a dichos tipos.\n",
    "\n",
    "El siguente enlace apunta a la referncia de tipos de datos de *Spark*.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda importará todos los tipos de datos de *PySpark* al entorno de esta *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de *schemas* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *schemas* describen estructuras de tipos de datos en *Spark*. Estas estructuras no sólo describen tablas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El tipo ```pyspark.api.types.StructField```.\n",
    "\n",
    "El tipo [```pyspark.api.types.StructField```](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.StructField.html) permite definir esquemas tanto para describir las columnas de un *dataframe* como para describir *schemas* complejos como los que se pueden encontrar en *YAML* o *JSON*.\n",
    "\n",
    "```\n",
    "StructField(<identificador>, <tipo>, <nulificable>)\n",
    "```\n",
    "Donde:\n",
    "\n",
    "* ```<identificador>``` es el nombre del campo.\n",
    "* ```<tipo>``` es el tipo de dato del campo.\n",
    "* ```<nulificable>``` es un valor booleano que indica si el campo puede aceptar valores nulos.\n",
    "\n",
    "\n",
    "Las estructuras complejas que pueden crearse en los *schemas* son contenidas dentro de un objeto de tipo [```pyspark.api.types.StructType```](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.StructType.html).\n",
    "\n",
    "```\n",
    "StructType(<campo 1>, <campo 2>, ... , <campo n>)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ```<campo i>``` es un objeto de tipo ```StructField```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda describe un *schema* para el *dataframe* ```df``` en el que la columna ```Pasajeros``` es un entero que va de ```-128```a ```127```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Dirección', StringType(), True), \n",
    "                     StructField('Rumbo', StringType(), True), \n",
    "                     StructField('Pasajeros', ByteType(), True), \n",
    "                     StructField('Documentado', BooleanType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará una *dataframe* a partir de ```pandas_df``` con el *schema* correspondiente a ```schema```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=pandas_df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y escritura de archivos para *dataframes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lectura de datos hacia un *dataframe*.\n",
    "\n",
    "En el caso de *PySpark*  es posible leer y crear *dataframes* a partir de distintos formatos de archivo que describan estructuras tabulares. El objeto [```spark.read```](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.read.html) contiene una familia de métodos que pueden importar y convertir en *dataframes* diversos tipos de documento y de diversas fuentes.\n",
    "\n",
    "```\n",
    "df = spark.read.<método>(<ruta>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El método ```spark.read.option()```.\n",
    "\n",
    " El método ```spark.read.options()``` permite configurar mediante los argumentos que se ingresen, ciertas restricciones y funcionalidades propias del *dataset* a importar.\n",
    " \n",
    " ```\n",
    " spark.read.option(\"<nombre 1>\", \"<valor 1>\").option(\"<nombre 2>\", \"<valor 2>\")....<metodo>(<ruta>)\\\n",
    " ```\n",
    " \n",
    " Donde:\n",
    " \n",
    " * ```<nombre i>``` es el nombre de cierta configuración aplicable al método de importación.\n",
    " * ```<valor i>``` es el valor de la configuración de ```<nombre i>```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El código siguiente le indica a *Spark* que importe un *dataset* contenido en un archivo *CSV* que tiene por delimitado al caracter ```;``` y que también contiene una línea de encabezados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura de datos desde un *dataframe*.\n",
    "\n",
    "En el caso de *PySpark*  es posible leer y crear dataframes a partir de distintos formatos de archivo que describan estructuras tabulares. Las siguientes propiedades de los *dataframes* permiten exportar dataframes a distintos formatos y destinos.\n",
    "\n",
    "* [```df.write```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.write.html) perimte crear y escribir una serie de documentos en el fromato indicado.\n",
    "* [```df.writeTo```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.writeTo.html) permite añadir (*append*) datos a un documento o estructura existente.\n",
    "* [```df.writeStream```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.writeStream.html) permite enviar los datos del *dataframe* a un flujo.\n",
    "\n",
    "```\n",
    "df.write.<método>(<ruta>)\n",
    "```\n",
    "La propiedad ```df.write``` es una implementación del objeto [```DataFrameWriter```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html#pyspark.sql.DataFrameWriter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fuentes de datos compatibles para los *dataframes*.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### El método ```spark.read.parquet()```.\n",
    " \n",
    " [*Apache Parquet*](https://parquet.apache.org/) es un formato binario capaz de almacenar estructuras de datos y conservar sus *schemas*.\n",
    " \n",
    " El siguiente enlace apunta a la documentación de la lectura/escritura de documentos en formato *parquet* para *Spark*.\n",
    " \n",
    " https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda importará los datos del archivo ```data/data_covid.parquet```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/data_covid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método ```df.toPandas()```.\n",
    "\n",
    "El método ```df.toPandas()``` permite crear un *dataframe* de *Pandas* a partir de un *dataframe* de *Apache Spark*.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación se usará el método ```df.toPandas()``` para mostrar el *dataframe* ```df``` de mejor manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### El método ```spark.read.csv()```.\n",
    " \n",
    " Los documentos *CSV* *(comma separated values)* son documentos de texto en los que se ingresan datos usando un caracter como separador. Por lo general, este separador es el caracter ```,```. Los documento CSV no conservan el tipo de dato y todos los elmentos son cadenas de caracteres. *Spark* puede leer este tipo de formatos, pero es necesario indicarle el *schema* que utilizará.\n",
    " \n",
    " El siguiente enlace apunta a la documentación de la lectura/escritura de documentos en formato *CSV* para *Spark*.\n",
    " \n",
    " https://spark.apache.org/docs/latest/sql-data-sources-csv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda leerá los datos dentro del archivo [```data/data_covid.csv```](data/data_covid.csv). En este caso sólo se indica la opción de que existe un encabezado en el documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").csv('data/data_covid.csv')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda leerá los datos dentro del archivo [```data/data_covid.csv```](data/data_covid.csv). En este caso se indican las opciones de que existe un encabezado en el documento y que se inferirá el *schema* de las columnnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\", \"true\").csv('data/data_covid.csv')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda definirá un *schema* para el *dataframe* resultante de extraer el archivo *CSV*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('index', DateType(), True), \n",
    "                     StructField('AGUASCALIENTES', LongType(), True), \n",
    "                     StructField('BAJA CALIFORNIA', LongType(), True), \n",
    "                     StructField('BAJA CALIFORNIA SUR', LongType(), True), \n",
    "                     StructField('CAMPECHE', LongType(), True), \n",
    "                     StructField('CHIAPAS', LongType(), True), \n",
    "                     StructField('CHIHUAHUA', LongType(), True), \n",
    "                     StructField('DISTRITO FEDERAL', LongType(), True), \n",
    "                     StructField('COAHUILA', LongType(), True), \n",
    "                     StructField('COLIMA', LongType(), True), \n",
    "                     StructField('DURANGO', LongType(), True), \n",
    "                     StructField('GUANAJUATO', LongType(), True), \n",
    "                     StructField('GUERRERO', LongType(), True), \n",
    "                     StructField('HIDALGO', LongType(), True),\n",
    "                     StructField('JALISCO', LongType(), True),\n",
    "                     StructField('MEXICO', LongType(), True),\n",
    "                     StructField('MICHOACAN', LongType(), True),\n",
    "                     StructField('MORELOS', LongType(), True),\n",
    "                     StructField('NAYARIT', LongType(), True),\n",
    "                     StructField('NUEVO LEON', LongType(), True),\n",
    "                     StructField('OAXACA', LongType(), True),\n",
    "                     StructField('PUEBLA', LongType(), True),\n",
    "                     StructField('QUERETARO', LongType(), True),\n",
    "                     StructField('QUINTANA ROO', LongType(), True),\n",
    "                     StructField('SAN LUIS POTOSI', LongType(), True),\n",
    "                     StructField('SINALOA', LongType(), True),\n",
    "                     StructField('SONORA', LongType(), True), \n",
    "                     StructField('TABASCO', LongType(), True), \n",
    "                     StructField('TAMAULIPAS', LongType(), True), \n",
    "                     StructField('TLAXCALA', LongType(), True), \n",
    "                     StructField('VERACRUZ', LongType(), True), \n",
    "                     StructField('YUCATAN', LongType(), True), \n",
    "                     StructField('ZACATECAS', LongType(), True), \n",
    "                     StructField('Nacional', LongType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").schema(schema).csv('data/data_covid.csv')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celda escribirán los datos del *dataframe* en diversos formatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('datos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('datos_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json('datos_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf datos_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option('encoding', 'UTF-8').json('datos_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nuevo = spark.read.option('encoding', 'UTF-8').json('datos_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de métodos de los *dataframes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index','AGUASCALIENTES').where(df.AGUASCALIENTES >1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index','AGUASCALIENTES').where(df.index == date(2020,12,20)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index','AGUASCALIENTES').filter(df.index >= date(2020,12,1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index', 'AGUASCALIENTES').sort('AGUASCALIENTES').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index', 'AGUASCALIENTES').sort('AGUASCALIENTES', ascending=False).limit(12).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index', 'AGUASCALIENTES', 'Nacional').sort('AGUASCALIENTES', ascending=False).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index', 'AGUASCALIENTES', 'Nacional').sort('Nacional', ascending=False).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags = df.select('index', 'AGUASCALIENTES').sort('AGUASCALIENTES', ascending=False).limit(10)\n",
    "dias_nacional = df.select('index', 'Nacional').sort('Nacional', ascending=False).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags.join(dias_nacional, 'index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags.join(dias_nacional, 'index').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags.join(dias_nacional, 'index', how='full').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags.join(dias_nacional, 'index').select('index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2023.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
